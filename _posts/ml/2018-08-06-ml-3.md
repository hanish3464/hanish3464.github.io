---
layout: post
title: 기계학습(ML)- Week 2
date:   2018-08-06
categories : [ML]
comments: true
---
# Machine Learning Coursera 2주차 


![ ](https://user-images.githubusercontent.com/51018265/71775466-a49bf080-2fc4-11ea-876f-886c0da3c724.png)

N과 m을 혼동하지 말아야 합니다. N은 “변수의 수”를 나타내고, m은 training set의 수를 나타냅니다. 지금까지 배운 hypothesis 함수의 형태는 H(x) = 세타0 + 세타1x 의 형태였습니다.
그렇다면, 여러 개의 변수일 경우의 H의 형태는 어떠할까요?


![ ](https://user-images.githubusercontent.com/51018265/71775467-a49bf080-2fc4-11ea-9b9e-d34092af9bf9.png)

Notation을 보다 편리하게 하면,

![ ](https://user-images.githubusercontent.com/51018265/71775468-a5348700-2fc4-11ea-947b-1db4bf1e209e.png)

우선, x0= 1 로 정의 합니다. 그리고 X 와 세타를 백터로 정의하고, 세타의 벡터를 전치행렬로 바꾸고 둘을 곱해주면 아래와 같은 식으로 추론이 된다는 것을 알 수 있습니다. 이것이 여러 개의 features(변수)를 갖고 있을 때의 hypothesis 함수의 형태이며,
이것을 multivariate linear regression(다변량 선형 회귀) 이라고 합니다.


어떻게 추론의 파라미터를 지정할 수 있을까요? – gradient descent 알고리즘 사용.

![ ](https://user-images.githubusercontent.com/51018265/71775469-a5348700-2fc4-11ea-9a82-2929c2865a6e.png)

Gradient descent 알고리즘이 다변량 선형 회귀에 적용 되었을 때의 식 입니다. 왜 이렇게 도출 됐는지 알아 두어야 합니다. H(x) = 세타0 + 세타1 * x 에서 
변수가 다량으로 늘어나면서 h(x) = 세타 0 + 세타1 *x + 세타2 * x2 + … + 세타n * xn으로 늘어났음을 알 수 있습니다. 즉, 이것은 변수의 수가 늘어난 것인데요.
세타 0를 gradient descent 알고리즘을 통해 값을 나타낼 때 기존의 세타0 – 알파(훈련비율) * cost function의 기울기 입니다. 세타0와 세타1의 경우 한정했을 때, 뒤에 x0와 x1이 생략된 것을 의미 합니다.
X0는 값이 1이기 때문에 굳이 쓰지 않은 것이며, 따라서 세타j 를 gardient descent 알고리즘으로 나타냈을 때는 j번째 변수에 해당하는 x의 j번째 변수의 weight를 빼서 적합한 regression model로 접근해 가겠다는 의미로 해석 할 수 있습니다.

### Feature Scaling 은 무엇일까요? 
서로 다른 feature 의 범위를 0~1 쯤으로 수정하는 것이고 이를 통해 gradient descent 알고리즘을 더욱 빠르게(적은 수의 반복) 접근 시킬 수 있는 방법 입니다.

![ ](https://user-images.githubusercontent.com/51018265/71775470-a5348700-2fc4-11ea-86b5-0d25f0bee10b.png)

위의 그림에서 세타1과 세타2에 해당하는 등고선 그래프를 보겠습니다. Feature에 대한 scaling이 이루어지지 않는다면 극단적으로 얇고 뾰족하여 엄청 오랜 시간 끝에 최소값을 찾아가게 될 것 입니다. (세타1의 범위가 세타 2의 범위보다 훨씬 크므로 촘촘한 그래프가 만들어지기 때문.)
따라서, size는 2000으로 나누고 bedroom의 수는 5로 나눔으로써 변수의 범위를 0~1로 바꾸면 훨씬 빠른 속도로 최소값을 찾아 갈 수 있게 됩니다. 범위는 너무 작지도 않고 크지도 않는 0~1 사이에서가 가장 좋다고 합니다.


### Feature Scaling 의 방법은?

![ ](https://user-images.githubusercontent.com/51018265/71775471-a5348700-2fc4-11ea-8875-3d78f6469426.png)

뮤1 의 값은 training set의 평균 값이고 s1 은 범위에서 max – min 또는 표준편차로 하고 계산 합니다. 대략 이를 통해 나온 값은 근사값이고 결국 목적은 gradient descent 알고리즘을 보다 빠르게 최소값에 접근하기 위해 사용하는 방법이므로 너무 정확할 필요는 없는 정규화 식을 이용합니다.

### Gradient descent 를 위한 유용한 방법은?

Learning rate에 대해서 이야기 해보겠습니다.

1. debugging

2. learning rate 알파를 고르는 방법

![ ](https://user-images.githubusercontent.com/51018265/71775472-a5cd1d80-2fc4-11ea-8c23-819b4298d4b6.png)

1,2번을 알아보기 전에 위의 그래프를 보겠습니다. x축은 gradient descent 의 반복횟수이고 y축은 cost function의 값에 해당합니다. 즉, 그래프를 통해 gradient descent 알고리즘을 반복하여 cost function 의 최소값인 0을 향해 가고 있음을 알 수 있습니다. 각각 100번, 200번 반복했을 때의 cost function의 값을 알 수 있고, 300~400번 반복할 때 거의 값의 변화가 없음을 알 수 있습니다.
이때 cost function의 감소하는 값이 어떤 값 입실론 0.001 보다 작을 경우 수렴했다고 판단하여 멈추게 됩니다. 이것은 automatic covergence test 라고 하는데 잘 쓰지는 않습니다.

![ ](https://user-images.githubusercontent.com/51018265/71775473-a5cd1d80-2fc4-11ea-9baf-de32fe53a9b5.png)

이런식으로 알파의 값이 너무 클 때 오히려 cost function 의 그래프가 반복함에 따라 감소하는 것이 아니라 증가하게 됩니다. 반대로 알파의 값이 너무 작을 때 아주 많은 반복을 통해
최소값을 찾아가는 그래프가 될 수도 있습니다.

요약을 하자면,

![ ](https://user-images.githubusercontent.com/51018265/71775474-a5cd1d80-2fc4-11ea-8b67-57d7e798a0d3.png)


변수가 여러 개인 linear regression 을 배웠습니다. 이번엔 feature 을 간단하게 선택하는 방법과 적절한 feature 의 선택으로 강력한 learning 알고리즘을 배우는 방법을 배워보도록 하겠습니다.
또한 polynomial regression 를 배울텐데 linear regression 을 이용하여 복잡한 비선형 함수에도 적용하는 것을 말합니다.

![ ](https://user-images.githubusercontent.com/51018265/71775475-a665b400-2fc4-11ea-8193-0d889b153328.png)

이러한 data set이 주어졌다고 가정합니다. 1차식에 맞는 regression model 이 적합해 보이지 않으므로 2차식과 3차식을 생각해 보았습니다. 그러나 2차함수 역시 결국 감소하므로 3차식이 보다 정확해 보입니다. 
이때 h(x) = 세타0 + 세타1*x1 + 세타2*x2 + 세타3*x3 일때 x1 은 size x2 는 size 의 제곱 x3은 size의 세제곱이 되도록해야 3차식으로 regression model을 구현할 수 있습니다. 
또한 사이즈의 크기에 따라 feature scaling 을 통해 적절한 범위를 만들어야 gradient descent 알고리즘을 보다 효율적으로 사용할 수 있음을 알 수 있습니다.

![ ](https://user-images.githubusercontent.com/51018265/71775476-a665b400-2fc4-11ea-86d4-f410d59fb72a.png)

이런식의 루트를 기반으로 함수를 만들 수도 있습니다.


2차식 3차식. 즉, 다항식 기반으로한 polynomial regression에 대해서 배웠습니다. 다시말해 2차함수나 3차함수를 데이터에 알맞게 쓰는지를 배우게 되는데
또한 어떻게 feature을 알맞게 쓰는지에 대해서 배웁니다. 예를들어 frontage 와 depth 라는 2가지 변수가 아닌 area라는 하나의 변수로 사용하는 것 입니다.
여러 feature가 있을 때 무엇을 사용해야하는지 헷갈립니다. 따라서 자동으로 feature 를 골라주는 알고리즘에 대해서 배워보도록 하겠습니다.

### Normal equation 에 대해서…
한번의 실행으로 최적값을 구하는 것 (not 여러 번 반복 = gradient descent)

![ ](https://user-images.githubusercontent.com/51018265/71775477-a665b400-2fc4-11ea-88c1-7fea4ebf8649.png)

세타가 1차원일 때는 위와같이 이차식으로 나오고, 이를 미분하여 세타가 =0 이 나오는 값이 최소임을 알 수 있습니다. 그러나 중요한 것은 세타가 n+1차원 파라미터 벡터일 경우에 해당하는
다량 변수의 경우입니다. 이럴 경우는 해당 세타j에 대하여 *편미분*을 하게 됩니다. 따라서 이를 통해 각각에 해당하는 세타j에 대해 최소값을 구할 수 있습니다.

![ ](https://user-images.githubusercontent.com/51018265/71775478-a665b400-2fc4-11ea-8c42-afbad01f3fd2.png)

### <normal equation>

필자는 증명이나 확실한 이해는 할 수는 없으나 구하고자 하는 세타의 값이 위와 같이 x transfer * x 에 inverse하고 * x transfer * y 를 하면 구할 수 있습니다.
Normal equation은 gradient descent와는 달리 feature scaling의 방식이 필요 없습니다. 왜냐하면 한번에 최적값을 찾아가는 방식이기 때문 입니다.


그렇다면 세타를 구하기 위해 언제 gradient descent와 normal equation을 사용해야 할까요?

![ ](https://user-images.githubusercontent.com/51018265/71775479-a6fe4a80-2fc4-11ea-858c-a9c0e46bc433.png)


<각 알고리즘의 장단점>


![ ](https://user-images.githubusercontent.com/51018265/71775480-a6fe4a80-2fc4-11ea-9606-74a431ae5b96.png)


Gradient descent 는 알파(Learning rate)값의 선정과 많은 반복이 필요하다라는 단점이 존재 하지만, feature 의 수가 굉장히 많을 때 잘 작동한다는 장점이 있다.

Vs

Normal equation 은 알파와 반복이 필요없이 단한번에 식으로 최적값이 간다는 장점이 있습니다. 그러나 X transfer * X 의 역행렬을 구하는 것은 보통 시간복잡도 O(n^3)에 해당합니다.
따라서 feature수가 많아 질수록 구하는 속도가 현저히 떨어지게 됩니다. 대충 feature의 수가 1만개를 넘기면 사용하는 것을 고려해봐야 합니다.

그렇다면 normal equation 에서 X transfer * X 가 계산 불가능한 경우는 언제이며 무슨 경우일까요?

![ ](https://user-images.githubusercontent.com/51018265/71775481-a6fe4a80-2fc4-11ea-8d94-19158a2ce6e3.png)

대개는 계산이 잘 됩니다. 그러나

1. 위처럼 feet 나 m 처럼 중복될 수 있는 feature 의 경우에는 계산이 불가능 합니다.

2. 많은 feature의 수. 즉, n 의 개수가 training set. M 보다 많은 경우에 불가능 합니다.

왜냐하면 많은 n을 굉장히 적은 m을 계산하기엔 data의 수가 너무 적기 때문 입니다. 따라서 feature n의 개수를 줄이거나 regularization이라는 방법을 사용해야 합니다.

### What is vectorization? 공식의 행렬화.

![ ](https://user-images.githubusercontent.com/51018265/71775482-a6fe4a80-2fc4-11ea-9d82-19567fe39335.png)

Unvectorization은 theta * x 의 시그마합으로 계산합니다. 코드로는 for문을 통해 prediction에 계속 더해가는 방식입니다.

그러나 vectorization은 theta 의 transfer * x 방식으로 계산합니다. (간단하고 훨씬 효율적)

![ ](https://user-images.githubusercontent.com/51018265/71775483-a796e100-2fc4-11ea-86d7-27310c26ceef.png)


위에 나온 식은 Gradient descent 알고리즘의 식입니다. 이런식으로 수식을 통해 코드적으로 구현해내려면 위와 같이 세타0,1,2 를 나눠서 구해야 합니다.
왜냐하면 알고리즘 내에 시그마의 합이 있기 때문 입니다. 그러나 이를 vectorization 해보도록 하겠습니다.


세타0,1,2 를 모두 세타라고 하고 세타 := 세타 – 알파* 델타라고 정의할 때, 세타는 n+1 벡터이고 알파는 스칼라 실수이며, 델타역시 n+1 벡터 입니다.

이때, 델타를 좀 더 구체적으로 보면, 델타는 h(x) – y * x 의 시그마 합 입니다. 또한 x 같은 경우는 n+1 차원 벡터 입니다. 즉 시그마의 합을 하나씩 나열 하되, h(x)- y*x 의 값을 R1, R2, R3, R4 … Rm 이라고 하면, R1 * (x1+x2+x3+ … +xn) 이고 R2 * (x1+x2+x3+ … +xn), R3 * (x1+x2+x3+ … +xn) … RM * (x1+x2+x3+ … +xn) 의 식이 됩니다.
따라서 결국 델타라는 벡터의 index 는 R1 * (x1+x2+x3+ … +xn) 부터 ~ RM * (x1+x2+x3+ … +xn) 까지의 m차원 백터로 이루어지게 됩니다. 이런식으로 수식을 한번에 백터를 이용하여 해결하는 것을 vectorization이라 합니다.
