---
layout: post
title: 기계학습(ML)- Week 1
date:   2018-07-30
categories : [Machine Learning]
comments: true
---
# Machine Learning Coursera 1주차 

![ ](https://user-images.githubusercontent.com/51018265/71764033-33a5fb80-2f26-11ea-96a2-419b74d369bd.png)

이 직선은 사이즈에 대한 가격 변화를 잘 나타내주며 일차방정식의 형태로 표현이 가능할 것 같습니다.

앞으로,

m = 학습 예제의 수

x = feature 부르는 입력 변수

y = 출력값이나 예측하려하는 목표변수

I = i번째 학습예제

로 정의하고자 합니다.

위 그림의 직선은 사이즈에 따른 집의 가격을 의미하는데 이 직선을 통해 사이즈라는 x 의 값으로 집 값이라는 y의 값을 예측 해 낼 수 있습니다. 따라서 위와 같은 직선을 찾는 것이 우리의 목표가 될것이며, 이를 우리가 직접 찾는 것이 아니라 머신러닝 알고리즘의 학습을 통하여 찾게 되는 것을 목표로 합니다.


### H(hypothesis) 는 x에서부터 y까지의 지도이다.

그렇다면 가설 H에 대해서 어떻게 표현할 수 있을까요? X의 선형함수인 y를 예측하는 것 입니다. H(x) = 세타1 + 세타2 * x 로 나타내며, Hypothesis function을 잘 세우기만 하면 x라는 입력값에 대해서 y라는 우리가 원하는 집 값을 결과 값으로 도출 해낼 수 있음을 의미 합니다.


![ ](https://user-images.githubusercontent.com/51018265/71764034-343e9200-2f26-11ea-9e9b-60808b3ab8b7.png)

이러한 특정 모델을 linear regresssion 이라하며, 보다 구체적으로는 단일변량(one variable) linear regression 이라 합니다.


H(x) = Wx + b 라고 할 때, H 함수는 정해지지 않는 W와 b 역시 변수로써 찾아가는 과정이고 이를 통해 찾아진 구체적인 3x + 2 와 같은 함수가 linear regression model 이라고 할 수 있습니다.


Supervised learning을 배우고 있기 때문에 예측 결과 값과 실제 값을 비교할 수 있습니다. 이것을 비교 할 수 있는 표현이 cost 인데, 지금부터 cost를 수학적인 함수로서 표현하는 방식을 배워보도록 하겠습니다.

### Cost funcition

![ ](https://user-images.githubusercontent.com/51018265/71764035-343e9200-2f26-11ea-9096-e689e1a12580.png)

이렇듯 H(x). 즉, Hypothesis 함수는 세타 값(Weight 와 bias)에 따라 달라진다는 것을 알 수 있습니다. 즉, Hypothesis 함수의 핵심은 얼마나 실제 데이터와 잘 일치하는지 라는 것을 알 수 있으며, 차이를 최소화 하는 것이 중요하다는 것을 알 수 있습니다. 또 다른 말로는 세타 값에 해당하는 weight와 bias를 잘 찾아내는 것을 의미 합니다.


i=0 ~ m 까지 (H(x) – y )^2 * 1/m 의 최소값을 구하는 것이 아래의 그림에 대한 설명입니다.

![ ](https://user-images.githubusercontent.com/51018265/71764037-343e9200-2f26-11ea-9703-8530c1ff9add.png)

즉, 세워진 hypothesis 함수를 실제 레이블 결과 값과 비교하여 뺀 것은 둘 간의 차이를 의미하여, 이것을 최소화 해야하는 cost로 정의 할 수 있습니다. 음수 값을 방지 하기 위해 제곱의 합이 최소화 되도록 정의하였으며, 나타낸 위의 식을 cost function 이라고 합니다. m개의 data에 대하여 평균을 내기 위해 2m으로 나눠 준 것 역시 식에 포함되어 있는데, 정리를 하자면 결국 cost를 최소화 시키기 위해서는 세타0와 세타1. 적절한 Weight와 bias 의 값을 찾아내는 것이 목표가 된다는 것을 알 수 있습니다. 여기서 cost function을 J로 나타내고 있고 이 J 함수를 minimize 된 값을 찾는 것이 목표가 됩니다.

![ ](https://user-images.githubusercontent.com/51018265/71764038-343e9200-2f26-11ea-9173-b164abe0710f.png)

Cost function을 보다 간단화하여 이해하기 위해 세타0(bias)를 0으로 가정한 그림 입니다. 실제 data가 왼쪽의 그림처럼 위치할 경우 h(x) = theta1 * x 에서 theta 1에 값이 따른 cost function의 그래프를 나타내면, 이차방적식의 그래프가 되고 이 그래프의 최소가 되는 값을 찾는 것이 목표가 됩니다. (미분한 값이 0이 되는 곳이 최소라는 것도 미루어 짐작 가능.)

#### 결론적으로, H(x)에서는 실제 데이터들에 가장 잘맞는 직선을 찾는 것이고 이는 결국 cost function 입장에선 최소의 값(미분 = 0)을 찾는 과정이 됩니다.

![ ](https://user-images.githubusercontent.com/51018265/71764039-34d72880-2f26-11ea-8cbf-27a9be130458.png)

지금까지 배운 것을 식으로 정리하면 위와 같습니다.

![ ](https://user-images.githubusercontent.com/51018265/71764040-34d72880-2f26-11ea-80b5-16b5f30d26a3.png)

3차원이 아닌 등고선 그래프를 통해 cost function 보고자 합니다. 위의 3개의 점은 같은 값을 가지고 있습니다.

먼저 오른쪽의 cost function 을 보면, x,y 축이 세타0, 세타1 임을 알 수 있습니다. 즉 왼쪽 그래프의 y절편(bias)과 기울기(weight)에 해당하는 값 입니다.

![ ](https://user-images.githubusercontent.com/51018265/71764041-34d72880-2f26-11ea-9dae-b8cf08a21cc1.png)


이처럼 cost function의 최소값에 향하고 실제 data에 알맞은 직선이 긋어지는 것에서 알맞은 parameter의 값을 찾은 것을 알 수 있습니다.

좋은 hypothesis function은 cost function의 그래프의 최소값과 비슷할수록 좋은 것이기도 합니다.


그렇다면 이것을 자동으로 구해주는 알고리즘이 있을까요?

### Gradient descent 알고리즘이란? 여러가지 함수의 최소값을 구하기 위해서 사용하는 알고리즘.

![ ](https://user-images.githubusercontent.com/51018265/71764042-34d72880-2f26-11ea-8807-f8459390692e.png)

:= 기호는 “할당하다”라는 의미를 가지고 있음. 즉 a := b 는 b값을 a에 덮어 씌우는 것을 말합니다.

= 기호는 단순히 a=b 일때, a와 b가 같다라는 진위형 문장을 나타냅니다.

따라서, 위의 식은 오른 쪽에 있는 식의 결과 값을 왼쪽 식에 새롭게 할당한다는 의미를 가집니다.

여기서 알파의 의미는 learning rate를 나타냅니다. 예를 들면 언덕에서 가장 낮은 최소 값을 가기 위해서 취하는 걸음을 의미 하는데, 알파의 값이 클수록 큰 보폭으로 찾아가는 것이고, 반대로 알파의 값이 작으면 작은 보폭으로 찾아가는 것을 의미합니다. 뒤에 있는 것은 미분을 의미합니다.

![ ](https://user-images.githubusercontent.com/51018265/71764043-356fbf00-2f26-11ea-82fe-8d6ed5029072.png)

코드의 구현에 있어서는 왼쪽처럼 먼저 temp 0 와 1 을 동시에 먼저 빼주고 값을 할당해야 합니다.

오른쪽처럼 할 경우는 세타 0의 값이 이미 바뀐상태로 temp1의 값에 영향을 미치게 되기 때문입니다. 이것이 주로 Gradient descent 에서 값을 대입하는 방식입니다.

다음으로 자세하게 다루지 못했던 미분계수에 부분에 대해서 알아 보겠습니다.

![image13](https://user-images.githubusercontent.com/51018265/71764043-356fbf00-2f26-11ea-82fe-8d6ed5029072.png)

이런식으로 미분계수를 빼게 됨으로써 결국 cost function의 최소값을 향해 간다는 것을 알 수 있습니다. 중앙의 최소를 찾기 위해서 기울기가 양수일땐 빼지고 음수일땐 더해지면서 중앙으로 간다는 것을 알 수 있습니다.


![ ](https://user-images.githubusercontent.com/51018265/71764045-356fbf00-2f26-11ea-875e-d830180207bd.png)

이런식으로 알파의 값이 너무 작으면 최소값을 찾기 까지 오랜시간이 걸리고, 반대로 알파의 값이 너무 크다면 최소값을 찾지못하고 발산하게 됩니다.

![ ](https://user-images.githubusercontent.com/51018265/71764046-356fbf00-2f26-11ea-8b77-e330cafb709d.png)

이런식으로 만약 지역적으로 최적화된 최소값을 찾으면 어떻게 될까요? 기울기가 0이 되기 때문에 결국 뺀값이 0가 되므로 더 이상 변하지 않게 됩니다.


Cost function에 최소값을 찾아가기 위해 알파의 값을 바꿔야 할까요? No


![ ](https://user-images.githubusercontent.com/51018265/71764047-36085580-2f26-11ea-9191-c6b027c8ed77.png)


Gradient descent (하강 기울기)는 지역 최소값이 가까워 질수록, 더 작은 거리를 이동하게 됩니다. 왜냐하면 위의 그래프처럼 미분계수의 값이 점점 작아질 것이기 때문입니다. 따라서 알파의 값을 의도적으로 감소시키지 않더라도 알아서 최소값을 향해 찾아가게 됩니다.

= 이것이 바로 Gradient Descent Algorithm 입니다.

그렇다면 다음으로 Linear regression 에서 사용되는 cost function으로 돌아가 Gradient Descent Algorithm 을 함께 사용해보도록 하겠습니다. 이를 통해 Learning Algorithm 을 이해하고 Linear Regression Algorithm 을 이해할 수 있을 것 입니다.

![ ](https://user-images.githubusercontent.com/51018265/71764048-36085580-2f26-11ea-8bad-3b6bb715963a.png)
![ ](https://user-images.githubusercontent.com/51018265/71764049-36085580-2f26-11ea-9ae1-def15cb38ef8.png)



![ ](https://user-images.githubusercontent.com/51018265/71764050-36085580-2f26-11ea-8670-32c8f8448535.png)

결국 위의 gradient descent 알고리즘은 기존의 parameter에 cost function의 편미분값을 빼는 것을 update를 반복하는 것 입니다. 이때, 세타0 와 세타1은 동시에 둘다 구해주어야 합니다.


그렇다면 gradient descent 알고리즘을 이용했을 때, H(x)와 cost function 에서 살펴보도록 하겠습니다.


![ ](https://user-images.githubusercontent.com/51018265/71764051-36a0ec00-2f26-11ea-9694-9adcb74d1823.png)

결국 알고리즘을 이용할수록 최적화된 최소값으로 이동하고 H(x)그래프 역시 데이터와 비슷한 형태로 변하게 됩니다.


Batch(집단) Gradient Descent 이라고 부르는 방식도 있는데 그 이유는 결국 m 개의 training set 을 이용하여 합계를 낸 것인데 batch 라는 말이 들어가는 머신러닝 용어입니다. 이 batch 라는 집단을 gradient 계산에 한번에 적용함으로써 성능적인 효율을 내기도 합니다.


### Linear Algebra – Matrices and vectors

What is vectors? 특수한 형태의 행렬을 말합니다. 하나의 열만을 가지는 행렬을 vector 라고 합니다

What is scalar? 실수라는 뜻, 여기에서는 상수 * 행렬 또는 상수 / 행렬에 대해서 배우게 됩니다.

변수 * 파라미터의 방식으로 활용되어 linear regression을 구현하는데 사용합니다.

![ ](https://user-images.githubusercontent.com/51018265/71764052-36a0ec00-2f26-11ea-94ce-411502c2d9c2.png)

이런식으로 행렬과 벡터를 이용하면 여러 개의 데이터와 H의 곱을 할 수 있습니다.

이렇듯 3개의 hypothesis 와 4개의 집 사이즈를 통해 12개의 가격을 예상하는 것에 빠르게 적용할 수 있음을 보여줍니다.
